{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install moviepy pydub librosa soundfile validators transformers torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from moviepy import VideoFileClip\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import validators\n",
    "\n",
    "def extract_audio(source, output_path=None, format=\"wav\"):\n",
    "    \"\"\"\n",
    "    Extract audio from a URL or local file path (audio or video).\n",
    "    \n",
    "    Args:\n",
    "        source (str): URL or file path of the audio or video file\n",
    "        output_path (str, optional): Path to save the extracted audio. If None, audio is not saved to disk.\n",
    "        format (str, optional): Format of the output audio file. Default is \"wav\".\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (audio_array, sample_rate) if output_path is None, otherwise the path to the saved audio file\n",
    "    \"\"\"\n",
    "    # Check if source is a URL or a file path\n",
    "    is_url = validators.url(source) or source.startswith(('http://', 'https://'))\n",
    "    \n",
    "    if is_url:\n",
    "        return extract_audio_from_url(source, output_path, format)\n",
    "    else:\n",
    "        # Check if it's a video or audio file\n",
    "        if source.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')):\n",
    "            return extract_audio_from_video(source, output_path, format)\n",
    "        elif source.lower().endswith(('.mp3', '.wav', '.ogg', '.flac', '.aac')):\n",
    "            return extract_audio_from_audio_file(source, output_path, format)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {source}\")\n",
    "\n",
    "def extract_audio_from_url(url, output_path=None, format=\"wav\"):\n",
    "    \"\"\"\n",
    "    Extract audio from a URL pointing to an audio or video file.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the audio or video file\n",
    "        output_path (str, optional): Path to save the extracted audio. If None, audio is not saved to disk.\n",
    "        format (str, optional): Format of the output audio file. Default is \"wav\".\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (audio_array, sample_rate) if output_path is None, otherwise the path to the saved audio file\n",
    "    \"\"\"\n",
    "    # Download the file\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to download file: {response.status_code}\")\n",
    "    \n",
    "    content_type = response.headers.get('content-type', '')\n",
    "    \n",
    "    # Process based on content type\n",
    "    if 'video' in content_type:\n",
    "        # Handle video file\n",
    "        temp_video_path = \"temp_video.mp4\"\n",
    "        with open(temp_video_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(temp_video_path)\n",
    "        audio = video.audio\n",
    "        \n",
    "        if output_path:\n",
    "            audio.write_audiofile(output_path)\n",
    "            video.close()\n",
    "            os.remove(temp_video_path)\n",
    "            return output_path\n",
    "        else:\n",
    "            temp_audio_path = \"temp_audio.wav\"\n",
    "            audio.write_audiofile(temp_audio_path)\n",
    "            audio_array, sample_rate = librosa.load(temp_audio_path, sr=None)\n",
    "            video.close()\n",
    "            os.remove(temp_video_path)\n",
    "            os.remove(temp_audio_path)\n",
    "            return audio_array, sample_rate\n",
    "            \n",
    "    elif 'audio' in content_type:\n",
    "        # Handle audio file\n",
    "        audio_data = BytesIO(response.content)\n",
    "        \n",
    "        try:\n",
    "            # Try loading with librosa\n",
    "            audio_array, sample_rate = librosa.load(audio_data, sr=None)\n",
    "            \n",
    "            if output_path:\n",
    "                sf.write(output_path, audio_array, sample_rate)\n",
    "                return output_path\n",
    "            else:\n",
    "                return audio_array, sample_rate\n",
    "                \n",
    "        except Exception:\n",
    "            # Fallback to pydub\n",
    "            audio = AudioSegment.from_file(audio_data)\n",
    "            \n",
    "            if output_path:\n",
    "                audio.export(output_path, format=format)\n",
    "                return output_path\n",
    "            else:\n",
    "                # Convert to numpy array\n",
    "                samples = np.array(audio.get_array_of_samples())\n",
    "                if audio.channels > 1:\n",
    "                    samples = samples.reshape((-1, audio.channels))\n",
    "                return samples, audio.frame_rate\n",
    "    else:\n",
    "        # Try to guess based on file extension\n",
    "        file_ext = url.split('.')[-1].lower()\n",
    "        if file_ext in ['mp4', 'avi', 'mov', 'mkv', 'flv', 'wmv']:\n",
    "            # Treat as video\n",
    "            temp_video_path = f\"temp_video.{file_ext}\"\n",
    "            with open(temp_video_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            return extract_audio_from_video(temp_video_path, output_path, format)\n",
    "        else:\n",
    "            # Try as audio\n",
    "            try:\n",
    "                audio_data = BytesIO(response.content)\n",
    "                return extract_audio_from_audio_file(audio_data, output_path, format)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Could not process content: {e}\")\n",
    "\n",
    "def extract_audio_from_video(video_path, output_path=None, format=\"wav\"):\n",
    "    \"\"\"\n",
    "    Extract audio from a local video file.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file\n",
    "        output_path (str, optional): Path to save the extracted audio. If None, audio is not saved to disk.\n",
    "        format (str, optional): Format of the output audio file. Default is \"wav\".\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (audio_array, sample_rate) if output_path is None, otherwise the path to the saved audio file\n",
    "    \"\"\"\n",
    "    # Load video\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    \n",
    "    if output_path:\n",
    "        # Save audio to file\n",
    "        audio.write_audiofile(output_path)\n",
    "        video.close()\n",
    "        return output_path\n",
    "    else:\n",
    "        # Return audio as numpy array\n",
    "        temp_audio_path = \"temp_audio.wav\"\n",
    "        audio.write_audiofile(temp_audio_path)\n",
    "        audio_array, sample_rate = librosa.load(temp_audio_path, sr=None)\n",
    "        video.close()\n",
    "        os.remove(temp_audio_path)\n",
    "        return audio_array, sample_rate\n",
    "\n",
    "def extract_audio_from_audio_file(audio_path, output_path=None, format=\"wav\"):\n",
    "    \"\"\"\n",
    "    Process an audio file (either from a file path or a BytesIO object).\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to the audio file or BytesIO object\n",
    "        output_path (str, optional): Path to save the processed audio. If None, audio is not saved to disk.\n",
    "        format (str, optional): Format of the output audio file. Default is \"wav\".\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (audio_array, sample_rate) if output_path is None, otherwise the path to the saved audio file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try loading with librosa\n",
    "        audio_array, sample_rate = librosa.load(audio_path, sr=None)\n",
    "        \n",
    "        if output_path:\n",
    "            sf.write(output_path, audio_array, sample_rate)\n",
    "            return output_path\n",
    "        else:\n",
    "            return audio_array, sample_rate\n",
    "            \n",
    "    except Exception:\n",
    "        # Fallback to pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "        \n",
    "        if output_path:\n",
    "            audio.export(output_path, format=format)\n",
    "            return output_path\n",
    "        else:\n",
    "            # Convert to numpy array\n",
    "            samples = np.array(audio.get_array_of_samples())\n",
    "            if audio.channels > 1:\n",
    "                samples = samples.reshape((-1, audio.channels))\n",
    "            return samples, audio.frame_rate\n",
    "\n",
    "# Example usage\n",
    "# 1. Extract audio from a URL\n",
    "# audio_data, sr = extract_audio(\"https://example.com/video.mp4\")\n",
    "# extract_audio(\"https://example.com/video.mp4\", \"output_audio.wav\")\n",
    "\n",
    "# 2. Extract audio from a local video file\n",
    "# audio_data, sr = extract_audio(\"input_video.mp4\")\n",
    "import os\n",
    "if os.path.exists(\"./file1.mp4\"):\n",
    "    extract_audio(\"./file1.mp4\", \"output_audio.wav\")\n",
    "else:\n",
    "    print(\"File does not exist: ./file1.mp4\")\n",
    "\n",
    "# 3. Extract audio from a local audio file\n",
    "# audio_data, sr = extract_audio(\"input_audio.mp3\")\n",
    "# extract_audio(\"input_audio.mp3\", \"output_audio.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import whisper\n",
    "\n",
    "# file_path = \"output_audio.wav\"\n",
    "\n",
    "# model = whisper.load_model(\"base\")\n",
    "# result = model.transcribe(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "ffmpeg_path = \"/opt/homebrew/bin/ffmpeg\"\n",
    "os.environ['PATH'] += f':{os.path.dirname(ffmpeg_path)}'\n",
    "\n",
    "file_path = \"output_audio.wav\"\n",
    "\n",
    "# Initialize the speech-to-text pipeline using Whisper\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "s2t_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-base\",\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Transcribe WAV audio file to text using Whisper model\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): Path to the WAV audio file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Transcription result with timestamps\n",
    "    \"\"\"\n",
    "    if not audio_path.endswith('.wav'):\n",
    "        raise ValueError(\"Only WAV files are supported\")\n",
    "        \n",
    "    result = s2t_pipe(audio_path)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if os.path.exists(\"output_audio.wav\"):\n",
    "    print(\"Transcribing audio...\")\n",
    "    transcription = transcribe_audio(\"output_audio.wav\")\n",
    "    print(\"Transcription result:\")\n",
    "    print(transcription)\n",
    "else:\n",
    "    print(\"Audio file not found. Make sure you have a WAV file to transcribe.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Определение структуры состояния\n",
    "class LectureSummaryState(TypedDict):\n",
    "    \"\"\"Состояние для процесса суммаризации лекции.\"\"\"\n",
    "    lecture_text: str\n",
    "    timing: str\n",
    "    analysis_result: str\n",
    "    summary_result: str\n",
    "    markdown_result: str\n",
    "    final_result: str\n",
    "\n",
    "# Функция для разбиения текста на части\n",
    "def chunk_text(text, max_chunk_size=16000):\n",
    "    \"\"\"Разбивает текст на управляемые фрагменты.\"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_size = len(paragraph)\n",
    "        \n",
    "        if current_size + paragraph_size > max_chunk_size and current_chunk:\n",
    "            chunks.append('\\n\\n'.join(current_chunk))\n",
    "            current_chunk = [paragraph]\n",
    "            current_size = paragraph_size\n",
    "        else:\n",
    "            current_chunk.append(paragraph)\n",
    "            current_size += paragraph_size + 2  # +2 for '\\n\\n'\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append('\\n\\n'.join(current_chunk))\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "class LectureSummarizer:\n",
    "    \"\"\"Класс для суммаризации лекций с LangGraph.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Инициализация LLM с указанными параметрами\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=os.getenv(\"OPENAI_MODEL_NAME\"),\n",
    "            temperature=0,\n",
    "            base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        )\n",
    "        \n",
    "        # Создание графа\n",
    "        self.graph = self._create_graph()\n",
    "    \n",
    "    def _analyze_lecture(self, state: LectureSummaryState) -> LectureSummaryState:\n",
    "        \"\"\"Анализирует текст лекции.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        ВАЖНО: Проанализируйте ТОЛЬКО предоставленный текст лекции.\n",
    "        \n",
    "        Текст лекции:\n",
    "        ```\n",
    "        {state['lecture_text']}\n",
    "        ```\n",
    "        \n",
    "        Информация о временных метках: {state['timing']}\n",
    "        \n",
    "        Ваша задача:\n",
    "        1. Определить основные темы и разделы ЭТОЙ лекции\n",
    "        2. Извлечь ключевые определения и понятия из текста\n",
    "        3. Определить, какие части подходят для таблиц\n",
    "        4. Отметить все временные метки\n",
    "        \n",
    "        НЕ ДОБАВЛЯЙТЕ информацию, которой нет в тексте.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.llm.invoke(prompt)\n",
    "        state[\"analysis_result\"] = result.content\n",
    "        return state\n",
    "    \n",
    "    def _summarize_content(self, state: LectureSummaryState) -> LectureSummaryState:\n",
    "        \"\"\"Создает краткое изложение разделов.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Создайте краткие изложения разделов лекции на основе анализа.\n",
    "        \n",
    "        Текст лекции:\n",
    "        ```\n",
    "        {state['lecture_text']}\n",
    "        ```\n",
    "        \n",
    "        Результаты анализа:\n",
    "        ```\n",
    "        {state['analysis_result']}\n",
    "        ```\n",
    "        \n",
    "        Временные метки: {state['timing']}\n",
    "        \n",
    "        Для каждого раздела создайте краткое изложение, сохраняя:\n",
    "        1. Ключевое содержание\n",
    "        2. Точные определения\n",
    "        3. Оригинальную структуру\n",
    "        4. Временные метки\n",
    "        \n",
    "        НЕ СОЗДАВАЙТЕ нового содержания.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.llm.invoke(prompt)\n",
    "        state[\"summary_result\"] = result.content\n",
    "        return state\n",
    "    \n",
    "    def _generate_markdown(self, state: LectureSummaryState) -> LectureSummaryState:\n",
    "        \"\"\"Создает Markdown-документ.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Создайте Markdown-документ на основе суммированного содержания.\n",
    "        \n",
    "        Текст лекции:\n",
    "        ```\n",
    "        {state['lecture_text']}\n",
    "        ```\n",
    "        \n",
    "        Краткие изложения:\n",
    "        ```\n",
    "        {state['summary_result']}\n",
    "        ```\n",
    "        \n",
    "        Документ должен включать:\n",
    "        1. Настоящее название лекции\n",
    "        2. Оглавление с разделами и временными метками\n",
    "        3. Отформатированные разделы с содержанием\n",
    "        4. Таблицы, где необходимо\n",
    "        \n",
    "        Пример оглавления:\n",
    "        ```markdown\n",
    "        # Основные философские течения в современном обществе\n",
    "        \n",
    "        ## Оглавление\n",
    "        1. [Традиционализм (0:21)](#традиционализм)\n",
    "        2. [Умеренный прогрессивизм: национальная школа (1:37)](#умеренный-прогрессивизм-национальная-школа)\n",
    "        3. [Умеренный прогрессивизм: глобальная школа (3:33)](#умеренный-прогрессивизм-глобальная-школа)\n",
    "        4. [Революционные движения (5:04)](#революционные-движения)\n",
    "        ```\n",
    "        \n",
    "        Верните ТОЛЬКО Markdown без комментариев.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.llm.invoke(prompt)\n",
    "        state[\"markdown_result\"] = result.content\n",
    "        state[\"final_result\"] = result.content  # Для одной части они совпадают\n",
    "        return state\n",
    "    \n",
    "    def _create_graph(self):\n",
    "        \"\"\"Создает граф обработки.\"\"\"\n",
    "        builder = StateGraph(LectureSummaryState)\n",
    "        \n",
    "        # Добавляем узлы\n",
    "        builder.add_node(\"analyzer\", self._analyze_lecture)\n",
    "        builder.add_node(\"summarizer\", self._summarize_content)\n",
    "        builder.add_node(\"markdown_generator\", self._generate_markdown)\n",
    "        \n",
    "        # Определяем последовательность\n",
    "        builder.add_edge(START, \"analyzer\")\n",
    "        builder.add_edge(\"analyzer\", \"summarizer\")\n",
    "        builder.add_edge(\"summarizer\", \"markdown_generator\")\n",
    "        builder.add_edge(\"markdown_generator\", END)\n",
    "        \n",
    "        # Компилируем граф\n",
    "        return builder.compile()\n",
    "    \n",
    "    def _merge_markdown_documents(self, markdown_parts):\n",
    "        \"\"\"Объединяет несколько Markdown-документов.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Объедините эти Markdown-документы в один целостный документ:\n",
    "        \n",
    "        ```\n",
    "        {markdown_parts}\n",
    "        ```\n",
    "        \n",
    "        Обеспечьте:\n",
    "        1. Единое оглавление со всеми разделами\n",
    "        2. Последовательное содержание без повторов\n",
    "        3. Согласованное форматирование\n",
    "        4. Сохранение всех временных меток\n",
    "        \n",
    "        Верните ТОЛЬКО объединенный Markdown без комментариев.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.llm.invoke(prompt).content\n",
    "    \n",
    "    def process_lecture(self, lecture_data, max_chunk_size=16000):\n",
    "        \"\"\"Обрабатывает лекцию с учетом таймингов.\"\"\"\n",
    "        # Подготовка данных\n",
    "        if isinstance(lecture_data, str):\n",
    "            chunks = chunk_text(lecture_data, max_chunk_size)\n",
    "            structured_chunks = [(chunk, None) for chunk in chunks]\n",
    "        elif isinstance(lecture_data, dict):\n",
    "            structured_chunks = list(lecture_data.items())\n",
    "        elif isinstance(lecture_data, list) and all(isinstance(item, tuple) for item in lecture_data):\n",
    "            structured_chunks = lecture_data\n",
    "        else:\n",
    "            raise ValueError(\"Неверный формат данных лекции\")\n",
    "        \n",
    "        # Если только одна часть\n",
    "        if len(structured_chunks) == 1:\n",
    "            text, timing = structured_chunks[0]\n",
    "            state = {\n",
    "                \"lecture_text\": text,\n",
    "                \"timing\": timing if timing else \"\",\n",
    "                \"analysis_result\": \"\",\n",
    "                \"summary_result\": \"\",\n",
    "                \"markdown_result\": \"\",\n",
    "                \"final_result\": \"\"\n",
    "            }\n",
    "            \n",
    "            result = self.graph.invoke(state)\n",
    "            return result[\"final_result\"]\n",
    "        \n",
    "        # Обработка нескольких частей\n",
    "        print(f\"Лекция разбита на {len(structured_chunks)} частей\")\n",
    "        markdown_parts = []\n",
    "        \n",
    "        for i, (text, timing) in enumerate(structured_chunks):\n",
    "            print(f\"Обработка части {i+1}/{len(structured_chunks)}...\")\n",
    "            \n",
    "            state = {\n",
    "                \"lecture_text\": text,\n",
    "                \"timing\": timing if timing else \"\",\n",
    "                \"analysis_result\": \"\",\n",
    "                \"summary_result\": \"\",\n",
    "                \"markdown_result\": \"\",\n",
    "                \"final_result\": \"\"\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                result = self.graph.invoke(state)\n",
    "                markdown_parts.append(result[\"markdown_result\"])\n",
    "                print(f\"Часть {i+1} обработана успешно\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке части {i+1}: {e}\")\n",
    "        \n",
    "        # Объединение результатов\n",
    "        if len(markdown_parts) > 1:\n",
    "            print(\"Объединение всех частей...\")\n",
    "            markdown_parts_text = \"\\n\\n===DOCUMENT SEPARATOR===\\n\\n\".join(markdown_parts)\n",
    "            final_markdown = self._merge_markdown_documents(markdown_parts_text)\n",
    "            return final_markdown\n",
    "        elif len(markdown_parts) == 1:\n",
    "            return markdown_parts[0]\n",
    "        else:\n",
    "            raise ValueError(\"Не удалось обработать ни одну часть лекции\")\n",
    "\n",
    "def summarize_lecture_with_timings(lecture_data, max_chunk_size=16000):\n",
    "    \"\"\"\n",
    "    Создает структурированный Markdown-документ с конспектом лекции.\n",
    "    \n",
    "    Args:\n",
    "        lecture_data: текст лекции (строка), словарь {текст: время} или список [(текст, время)]\n",
    "        max_chunk_size: максимальный размер фрагмента для обработки (по умолчанию 16000)\n",
    "        \n",
    "    Returns:\n",
    "        Markdown с суммаризацией лекции и таймингами\n",
    "    \"\"\"\n",
    "    summarizer = LectureSummarizer()\n",
    "    return summarizer.process_lecture(lecture_data, max_chunk_size=max_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OPENAI_BASE_URL=http://localhost:11434\n",
    "# %env OPENAI_MODEL_NAME=ollama/qwen2.5:latest\n",
    "%env OPENAI_BASE_URL=https://openrouter.ai/api/v1\n",
    "%env OPENAI_MODEL_NAME=qwen/qwen-2.5-72b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:11434\"\n",
    "# os.environ[\"OPENAI_MODEL_NAME\"] = \"ollama/qwen2.5:latest\"\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = \"https://openrouter.ai/api/v1\"\n",
    "# os.environ[\"OPENAI_MODEL_NAME\"] = \"openrouter/qwen/qwq-32b:free\"\n",
    "\n",
    "\n",
    "text = summarize_lecture_with_timings(transcription[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the summarized lecture text to a file\n",
    "with open(\"lecture_summary.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "    \n",
    "print(f\"Lecture summary saved to lecture_summary.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_openai\n",
    "langchain_openai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
